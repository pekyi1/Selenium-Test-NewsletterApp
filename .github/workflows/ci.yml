name: CI - Selenium UI Tests

on:
  push:
  pull_request:

jobs:
  ui-tests:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Java 17
        uses: actions/setup-java@v4
        with:
          distribution: temurin
          java-version: "17"
          cache: maven

      - name: Print target URL
        run: |
          echo "Testing URL: ${{ secrets.APP_BASE_URL }}"

      - name: Run Selenium tests (headless)
        run: mvn -B -e test -Dheadless=true -DbaseUrl=${{ secrets.APP_BASE_URL }} -DtrimStackTrace=false

      # Human-friendly summary in GitHub Actions logs (non-technical)
      - name: Human-friendly test summary (readable)
        if: always()
        run: |
          python3 - <<'PY'
          import glob, xml.etree.ElementTree as ET, os

          files = sorted(glob.glob("target/surefire-reports/TEST-*.xml"))
          if not files:
              print("No Surefire XML reports found (target/surefire-reports/TEST-*.xml).")
              raise SystemExit(0)

          total_tests = total_failures = total_errors = total_skipped = 0
          failed_details = []

          for f in files:
              root = ET.parse(f).getroot()
              suite_name = root.attrib.get("name", os.path.basename(f))
              tests = int(root.attrib.get("tests", 0))
              failures = int(root.attrib.get("failures", 0))
              errors = int(root.attrib.get("errors", 0))
              skipped = int(root.attrib.get("skipped", 0))

              total_tests += tests
              total_failures += failures
              total_errors += errors
              total_skipped += skipped

              for tc in root.findall("testcase"):
                  tname = tc.attrib.get("name", "(unnamed test)")
                  cname = tc.attrib.get("classname", suite_name)

                  failure = tc.find("failure")
                  error = tc.find("error")

                  if failure is not None:
                      msg = (failure.attrib.get("message") or "A check did not match the expected result.").strip()
                      failed_details.append(("FAIL", cname, tname, msg))
                  elif error is not None:
                      msg = (error.attrib.get("message") or "The test could not complete.").strip()
                      failed_details.append(("ERROR", cname, tname, msg))

          passed = total_tests - total_failures - total_errors - total_skipped

          print("\n==============================")
          print("SELENIUM UI TEST SUMMARY (Readable)")
          print("==============================")
          print(f"Total tests : {total_tests}")
          print(f"Passed      : {passed}")
          print(f"Failed      : {total_failures}")
          print(f"Errors      : {total_errors}")
          print(f"Skipped     : {total_skipped}")

          if failed_details:
              print("\nWhat failed and why")
              print("-------------------")
              for status, cls, name, msg in failed_details:
                  clean = " ".join(msg.replace("\\n", " ").split())
                  print(f"- {name} ({status})")
                  print(f"  Reason: {clean}")
          else:
              print("\nAll tests passed ✅")

          print("==============================\n")
          PY

      # Build Slack-friendly summary (non-technical)
      - name: Build Slack-friendly test summary
        if: always()
        id: slack_summary
        run: |
          python3 - <<'PY'
          import glob, xml.etree.ElementTree as ET, os

          files = sorted(glob.glob("target/surefire-reports/TEST-*.xml"))
          if not files:
              summary = "Selenium UI Tests finished, but no test report was found."
              with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
                  f.write("message<<EOF\n")
                  f.write(summary)
                  f.write("\nEOF\n")
              raise SystemExit(0)

          total_tests = total_failures = total_errors = total_skipped = 0
          failed_details = []

          for f in files:
              root = ET.parse(f).getroot()
              total_tests   += int(root.attrib.get("tests", 0))
              total_failures+= int(root.attrib.get("failures", 0))
              total_errors  += int(root.attrib.get("errors", 0))
              total_skipped += int(root.attrib.get("skipped", 0))

              for tc in root.findall("testcase"):
                  name = tc.attrib.get("name", "(unnamed test)")
                  failure = tc.find("failure")
                  error = tc.find("error")
                  if failure is not None:
                      msg = (failure.attrib.get("message") or "A check did not match the expected result.").strip()
                      failed_details.append((name, msg))
                  elif error is not None:
                      msg = (error.attrib.get("message") or "The test could not complete.").strip()
                      failed_details.append((name, msg))

          passed = total_tests - total_failures - total_errors - total_skipped
          status_word = "PASSED ✅" if (total_failures + total_errors) == 0 else "FAILED ❌"

          lines = []
          lines.append(f"*Selenium UI Tests {status_word}*")
          lines.append(f"Total: {total_tests} | Passed: {passed} | Failed: {total_failures} | Errors: {total_errors} | Skipped: {total_skipped}")

          if failed_details:
              lines.append("")
              lines.append("*What failed and why*")
              for (name, msg) in failed_details[:3]:
                  clean = " ".join(msg.replace("\\n", " ").split())
                  lines.append(f"• *{name}* — {clean}")
              if len(failed_details) > 3:
                  lines.append(f"• …and {len(failed_details) - 3} more (see workflow run for full details)")

          summary = "\n".join(lines)
          with open(os.environ["GITHUB_OUTPUT"], "a", encoding="utf-8") as f:
              f.write("message<<EOF\n")
              f.write(summary)
              f.write("\nEOF\n")
          PY

      # Technical reports for evidence (prints only when failing)
      - name: Print Surefire reports (on failure)
        if: failure()
        run: |
          echo "=== Surefire Reports Directory ==="
          ls -la target/surefire-reports || true

          echo "=== Dumping Surefire .txt reports (test failures/errors) ==="
          for f in target/surefire-reports/*.txt; do
            if [ -f "$f" ]; then
              echo "----- $f -----"
              cat "$f"
              echo
            fi
          done

      - name: Upload Surefire test reports
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: surefire-reports
          path: target/surefire-reports/**

      # Upload Selenium failure evidence (only exists if your tests write to target/test-evidence)
      - name: Upload UI failure evidence
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ui-test-evidence
          path: target/test-evidence/**

      - name: Notify Slack (non-technical summary)
        if: always()
        uses: slackapi/slack-github-action@v1.27.0
        with:
          payload: |
            {
              "text": "${{ steps.slack_summary.outputs.message }}\n\nRepo: `${{ github.repository }}` | Branch: `${{ github.ref_name }}`\nRun details: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
